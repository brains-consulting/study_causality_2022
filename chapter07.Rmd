---
title: |
  | 統計的因果推論の理論と実装
  |
  | Chapter7
  | 最小二乗法による重回帰モデルの
  | 仮定と診断 1
  |
  |
author: "大隈 亮"
date: '2022-04-20 (Wed)'
output: 
  revealjs::revealjs_presentation:
    #self_contained: false
    transition: none
    highlight: pygments
    theme: white
    reveal_options:
      #autoSlide: 5000
      width: 1280
      height: 960
      control: true
    css: "for-revealjs.css"
    pandoc_args: [
      '--from', 'markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures'
      ]
editor_options: 
  markdown: 
    wrap: 72
---

# Index

## Index {#index_1}

-   Chapter7の目的

-   7.1 仮定 1：誤差項の期待値ゼロ

-   7.2 仮定 2：パラメータ（母数）における線形性
    * 7.2.1 使用するデータの設定に関する情報
    * 7.2.2 線形モデルによる推定
    * 7.2.3 変数変換
    * 7.2.4 対数変換後の回帰係数の解釈
    * 7.2.5 変数の線形性と母数の線形性の違い
    * 7.2.6 多変量における診断方法

-   7.3 仮定 3：誤差項の条件付き期待値ゼロ
    * 7.3.1 不要な変数をモデルに取り入れる問題
    * 7.3.2 中間変数をモデルに取り入れる問題

# Chapter7の目的 {#Ch_7_0}

## Chapter7の目的 {#Ch_7_0_1}

第6章では、図を使って具体的に重回帰モデルのメカニズムを押さえた


第7章および第8章では、そこで置かれていた6つの過程を明示的に示す（Wooldridge, 2020, pp.79-96, pp.117-120; Fox, 2020, pp.6-7）


第7章では、以下の3つの仮定を明示的に示す

- 仮定1：誤差項の期待値はゼロという基本的な仮定

- 仮定2：パラメータ（母数）における線形性の仮定

- 仮定3：誤差項の条件付き期待値ゼロの仮定


仮定2と仮定3は、**重回帰モデルの回帰係数を偏りなく推定するために必要であり、極めて重要である**




# 7.1 仮定 1：誤差項の期待値ゼロ {#Ch_7_1}

## 7.1 仮定 1：誤差項の期待値ゼロ {#Ch_7_1_1}

第5章では、誤差項 $\varepsilon _{i}$ は期待値をとるとゼロ、つまり $E ( \varepsilon_{i} ) = 0$ と仮定すると述べた

誤差項 $\varepsilon _{i}$ は観測されないため、これは紛れもなく仮定ではあるものの、<br />
以下の通り、この仮定を満たすことは難しくない

<br />
$Y_{i} = \alpha_{0} + \beta_{1} X_{1i} + u_{i}$ というモデルを考える

$u_{i}$ はこのモデルにおける誤差項で、$E(u_{i}) = \delta$ で、$\delta \neq 0$ とする。

つまり、誤差項 $u_{i}$ の期待値はゼロではない場合を考える<br />
（浅野・中村, 2009, p.26; Wooldridge,  2020, p.58）


| 式変形 | 解説 |
|:---|:------|
|$Y_{i} = \alpha_{0} + \beta_{1} X_{1i} + u_{i}$|誤差項 $u_{i}$の期待値が0でないモデルから始めて、誤差項 $\varepsilon_{i}$ の期待値がゼロになることを示す.|
|$Y_{i} = \alpha_{0} + \beta_{1} X_{1i} + u_{i} + \delta - \delta$|右辺に $\delta$ を足して、$\delta$ を引く. $\delta - \delta = 0$ であるから、このような操作ができる|
|$Y_{i} = (\alpha_{0} + \delta) + \beta_{1} X_{1i} + (u_{i} - \delta)$|新たな $Y$ 切片は $\alpha_{0} + \delta$ であり、新たな誤差項は $u_{i} - \delta$ である.|
|$Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \varepsilon_{i}$|$\beta_{0} = \alpha_{0} + \delta$ 、 $\varepsilon_{i} = u_{i} - \delta$ と定義すると、馴染みのある式である.|
|$E[\varepsilon_{i}] = E[u_{i} - \delta]$|新たに定義しなおした誤差項 $\varepsilon_{i} = u_{i} - \delta$ の期待値をとる.|
|$E[\varepsilon_{i}] = E[u_{i}] - E[\delta]$|期待値の加法性 $E[X + Y] = E[X] + E[Y]$ より等号は成り立つ.|
|$E[\varepsilon_{i}] = \delta - \delta$|定義より、$E[u_{i}] = \delta$ である. また、$k$ を定数とするとき、$E[k] = k$ であり、$\delta$ は定数である|
|$E[\varepsilon_{i}] = 0$|よって、誤差項 $\varepsilon_{i}$ の期待値はゼロであるから、仮定1は満たされることがわかる.|

------------------------------------------------------------------------

すなわち、誤差項の期待値がゼロでない場合、

$Y$ 切片の値が $\alpha_{0}$ から $\beta_{0} = \alpha_{0} + \delta$ に変わるものの、傾きは $\beta_{1}$ のままであり、

新たな誤差項 $\varepsilon_{i} = u_{i} - \delta$ の期待値はゼロである

<br />
影響を受けるのは、$Y$ 切片の値だけであり、傾き $\beta_{1}$ には影響がなく、

誤差項 $\varepsilon_{i}$ の期待値はゼロとみなすことができる

-----------------------------------------------------------------------

表7.1

```{r echo=TRUE}
rm(list = ls())
set.seed(1); n1 <- 1000
a0 <- 1; b1 <- 1.5
x1 <- rnorm(n1)
u1 <- rnorm(n1, 10, 1)
y1 <- a0 + b1 * x1 + u1
model1 <- lm(y1 ~ x1)
summary(model1)
```

<br />
切片a0の値は10.984と推定されており、真値は１だったので、正しく推測できていない

しかし、傾きb1の値は1.506と推定されており、真値は1.5だったので、正しく推定されている


# 7.2 仮定 2：パラメータ(母数)における線形性 {#Ch_7_2}

------------------------------------------------------------------------

仮定2は回帰モデルにおけるパラメータ（母数）が線形という仮定である
<br />（Wooldridge, 2020, p.80）

この仮定は、**最小二乗法による回帰係数の推定量が不偏であるために必要であり、重要な仮定である**



## 7.2.1 使用するデータの設定に関する情報 {#Ch_7_2_1}

$\beta_{0} = 1.0$、$\beta_{1} = 1.5$ として、$\varepsilon_{i} \sim N(1, 0)$ とする

$X_{1i} \sim N(1, 0)$、$X_{2i} \sim LN(1, 0)$ として、2種類の説明変数 $X$ を用意する

つまり、$X_{1i}$ は正規分布に従っており、$X_{2i}$ は対数正規分布（lognormal distribution）に従っているとする

また、式（7.1）～式（7.4）のとおり、4種類の $Y$ を用意する

------------------------------------------------------------------------

- 式（7.1）：通常の線形モデル
- 式（7.2）：対数正規分布に従う $X_{2i}$ を自然対数（log）によって変換しており、$\log(X_{2i})$ は正規分布に従う
- 式（7.3）：右辺全体をexp(・)によって指数変換していることから、$Y_{3i}$ は対数正規分布に従っている
- 式（7.4）：すべての項が掛け算になっており、また、$e \approx 2.718$ は自然対数の底であり、$e^{\varepsilon_{i}} = \exp(\varepsilon_{i})$ である 

$$
\begin{align}
Y_{1i} & = \beta_{0} + \beta_{1} X_{1i} + \varepsilon_{i} \tag{7.1} \\
Y_{2i} & = \beta_{0} + \beta_{1} \log(X_{2i}) + \varepsilon_{i} \tag{7.2} \\
Y_{3i} & = \exp(\beta_{0} + \beta_{1} X_{1i} + \varepsilon_{i}) \tag{7.3} \\
Y_{4i} & = \beta_{0} X^{\beta_{1}} _{2i} e^{\varepsilon_{i}} \tag{7.4}
\end{align}
$$

式（7.1）～式（7.4）は、すべて、$Y$ に与える $X$ の平均処置効果（ATE）は $\beta_{1} = 1.5$ であることを意味している

それぞれにおいて、$\beta_{1} = 1.5$ をうまく推定できるかどうかが重要である


------------------------------------------------------------------------

表7.2

```{r echo=TRUE}
rm(list = ls())
data07a <- read.csv("./causality-main/data07a.csv")
summary(data07a)
```



## 7.2.2 線形モデルによる推定 {#Ch_7_2_2}

モデル1～モデル4は、表7.3のとおり、非線形性を無視して通常の最小二乗法（OLS）で回帰分析を行ったものである

表7.3
```{r echo=TRUE}
model1 <- lm(y1 ~ x1, data=data07a)
model2 <- lm(y2 ~ x2, data=data07a)
model3 <- lm(y3 ~ x1, data=data07a)
model4 <- lm(y4 ~ x2, data=data07a)
```

表7.4は出力結果を抜粋したものである

表7.4

| 　 | モデル1 | モデル2 | モデル3 | モデル4 |
|:---:|----:|----:|----:|----:|
|Y切片|0.988|-0.105|20.277|-3.352|
|傾き|1.506|0.661|36.446|5.235|


式（7.1）において、$X$ と $Y$ は線形の関係にあるため、モデル1では、$\hat{\beta_{1}}=1.506$ となっており、$\beta_{1} = 1.5$ を正しく推定できている様子がわかる

$Y$ 切片も 0.988 なので、$\beta_{0} = 1.0$ を正しく推定できている

一方、式（7.2）～式（7.4）では、$X$ と $Y$ は非線形の関係にあった

したがって、モデル2～モデル4では、それぞれ $\hat{\beta_{1}}=0.661$、$\hat{\beta_{1}}=36.446$、$\hat{\beta_{1}}=5.235$ となっており、$\beta_{1} = 1.5$ をうまく推定できていない様子がわかる  



## 7.2.3 変数変換 {#Ch_7_2_3}

モデル5～モデル7は、線形性を考慮するために変数変換を行ってから、通常の最小二乗法で回帰分析を行ったものである

表7.5
```{r echo=TRUE}
model5 <- lm(y2 ~ log(x2), data=data07a)
model6 <- lm(log(y3) ~ x1, data=data07a)
model7 <- lm(log(y4) ~ log(x2), data=data07a)
```

表7.6は出力結果を抜粋したものである

表7.6

| 　 | モデル5 | モデル6 | モデル7 |
|:---:|----:|----:|----:|
|Y切片|0.988|0.988|-0.012|
|傾き|1.550|1.506|1.550|


モデル5～モデル7では、それぞれ $\hat{\beta_{1}}=1.550$、$\hat{\beta_{1}}=1.506$、$\hat{\beta_{1}}=1.550$ となっており、

$\beta_{1} = 1.5$ を正しく推定できている様子がわかる

細かな違いは標本抽出誤差である


## 7.2.4 対数変換後の回帰係数の解釈 {#Ch_7_2_4}

対数変換を行った後の回帰係数は、解釈に注意が必要である

たとえば $\hat{\log(y_{i})} = \hat{\beta_{0}} + \hat{\beta_{1}} \log(x_{1i})$ というモデルの $\hat{\beta_{1}}$ は、

「$\log(x_{1i})$ が1変化した場合、平均的に、$\hat{\log(y_{i})}$ が $\hat{\beta_{1}}$ 変化する」

と形式的には解釈できるかもしれない

<br />
しかし、「$\log(x_{1i})$ が1変化」や「$\hat{\log(y_{i})}$ が $\hat{\beta_{1}}$ 変化」というのは、

単位も対数に変換されているため、実証研究ではあまり意味をなさない解釈である

そこで、対数変換後の回帰係数の解釈について確認しておこう

------------------------------------------------------------------------

対数変換後の回帰係数は、**パーセント変化を表している**と解釈できる

ただし、モデル内のどの変数が自然対数に変換されたかによって解釈の仕方が異なり、

表7.7のとおりである（Wooldridge, 2020, p.39, pp.374-677）

なお、表7.7の「単位」とは、たとえば、年収であれば日本円であったり、<br />
従業員数であれば人数であったり、変数の単位を表す

表7.7

| モデル | 解釈 |
|:---|:------|
|A：$\hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{1i}$|$x_{1}$ が1単位変化した場合、$y$ は $\hat{\beta_{1}}$ 単位変化する|
|B：$\hat{y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}} \log(x_{1i})$|$x_{1}$ が1%変化した場合、$y$ は $\hat{\beta_{1}}/100$ 単位変化する|
|C：$\hat{\log(y_{i})} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{1i}$|$x_{1}$ が1単位変化した場合、$y$ は $100\hat{\beta_{1}}$ %変化する|
|D：$\hat{\log(y_{i})} = \hat{\beta_{0}} + \hat{\beta_{1}} \log(x_{1i})$|$x_{1}$ が1%変化した場合、$y$ は $\hat{\beta_{1}}$ %変化する|

------------------------------------------------------------------------

第5章および第6章では、共分散分析によって平均処置効果の推定を行えることを確認した

式（7.5）を例として、処置 $T$ の効果を表す $\beta_{3}$ は、どのように解釈すればよいだろうか？

これは、表7.7のモデルCに該当する

つまり、共変量の影響を取り除くと、平均処置効果は $100 \hat{\beta_{3}}$ %である（Wooldridge, 2020, p.226）

$$
\log( Y_{i} ) = \beta_{0} + \beta_{1} \log(X_{1i}) + \beta_{2} X_{2i} + \beta_{3} T_{i} + \varepsilon_{i} \tag{7.5}
$$

<br />
<br />

上記のように、対数変換後の回帰係数をパーセント変化として解釈できる理由について、<br />
概略を解説する

対数関数 $\log(1 + x)$ をテイラー展開すると、式（7.6）の多項式で近似できることがわかる

$$
\log(1 + x) = x - \frac{1}{2} x^{2} + \frac{1}{3} x^{3} - \frac{1}{4} x^{4} + \cdots \tag{7.6}
$$

また、$x$ が極めて小さな値（$x 	\approx 0$）であれば、$\log(x + 1)	\approx x$ である<br />
（Wooldridge, 2020, p.675）

------------------------------------------------------------------------

表7.8
```{r echo=TRUE}
x1 <- 0.1
log(1 + x1)
x1 - 1/2 * x1^2 + 1/3 * x1^3 - 1/4 * x1^4
x1
```

<br />
2行目で $\log(1 + x)$ を直接計算しており、0.0953である

3行目では、式（7.6）により、4次までの多項式で近似している

ここでは、4次までの多項式の値は0.0953で、ほぼ完全に近似できている

4行目では、x1は0.1で0に近いので、x1だけの値0.1でもほぼ近似できていることがわかる

------------------------------------------------------------------------

次に、$y_{0} = 1.01$ とし、$y_{1} = 1.02$ としよう

つまり、$y_{0} = 1.00 + 0.01$、$y_{1} = 1.00 + 0.02$ である

$y_{0}$ から $y_{1}$ への伸び率は、式（7.7）のとおり、約0.99%である<br />
（迫田・高橋・渡辺編, 2014, p.16）

$$
\frac{y_{1} - y_{0}}{y_{0}} \times 100 = \frac{1.02 - 1.01}{1.01} \times 100 	\approx 0.0099 \times 100 = 0.99 \% \tag{7.7}
$$

<br />
また、対数法則より、式（7.8）が成り立つ

ここで、$x$ がきわめて小さな値（$x \approx 0$）であれば、$\log(1+x)	\approx x$ であることに注意すると、

$\log(y_{1}) = \log(1.00 + 0.02)	\approx 0.02$ であり、$\log(y_{1}) = \log(1.00 + 0.01)	\approx 0.01$

である

$$
\log(\frac{y_{1}}{y_{0}}) = \log(y_{1}) - \log(y_{0})	\approx 0.02 - 0.01 = 0.01 \tag{7.8}
$$

------------------------------------------------------------------------

表7.9
```{r echo=TRUE}
y0 <- 1.01
y1 <- 1.02
(y1 - y0) / y0
log(y1/y0)
log(y1) - log(y0)
log(1 + 0.02) - log(1 + 0.01)
```

<br />
log(y1) - log(y0) は0.00985で、ほぼ0.01であることがわかる

この値を100倍すると、1%であり、式（7.7）の結果とほぼ一致することから、

パーセント変化として解釈できることがわかる



## 7.2.5 変数の線形性と母数の線形性の違い {#Ch_7_2_5}

ここで、仮定2（パラメータにおける線形性）が禁止しているのは、

式（7.9）と式（7.10）のどちらであるかも確認しておく<br />
（Gujarati, 2002, pp.564-565）

<br />
これは、経済学において、コブ・ダグラス型生産関数と呼ばれるもので、

産出物 $Y$、資本 $X_{1}$、労働投入 $X_{2}$ の関係を表す式である<br />
（浅野・中村, 2009, p.97）

$$
\begin{align}
Y_{i} & = \beta_{0} X^{\beta_{1}} _{1i} X^{\beta_{2}} _{2i} e^{\varepsilon_{i}} \tag{7.9} \\
Y_{i} & = \beta_{0} X^{\beta_{1}} _{1i} X^{\beta_{2}} _{2i} + \varepsilon_{i} \tag{7.10}
\end{align}
$$

式（7.9）では、変数 $X_{j}$ が $\beta_{j}$ 乗になっている

また、誤差項 $\varepsilon_{i}$ は $e^{\varepsilon_{i}}$ であり、説明変数と掛け算になっている

これは、$X_{j}$ と $Y$ が非線形にあることを意味しているが、「変数における非線形性」である

<br />
仮定2（パラメータにおける線形性）では、**このようなモデルは禁止されていない**

------------------------------------------------------------------------

この問題を詳しく見る

ここで、log(・)は自然対数を表すこととする

また、$k$ を定数とすると、

$\log(A^{k}) = k \log(A)$ であり、$\log(AB) = \log(A) + \log(B)$

であることに注意する

<br />
式（7.9）の両辺の対数をとると、式（7.11）に変換できることがわかる

特に、$\log(e^{\varepsilon_{i}}) = \varepsilon_{i}$ の部分は重要である

$$
\log(Y_{i}) = \log(\beta_{0}) + \beta_{1} \log(X_{1i}) + \beta_{2} \log(X_{2i}) + \varepsilon_{i} \tag{7.11}
$$

<br />
式（7.11）では、変数$Y$、$X_{1}$、$X_{2}$ が自然対数に変換されただけであり、

単なる「変数における非線形性」であるため、

適切に変数変換（ここでは自然対数に変換）することで、

通常の最小二乗法によって $\beta_{j}$ を推定できる

------------------------------------------------------------------------

一方、式（7.10）では、変数 $X_{j}$ が $\beta_{j}$ 乗になっているだけでなく、

誤差項 $\varepsilon_{i}$ は足し算となっている

$\log(A + B) \neq \log(A) + \log(B)$ であるから、式（7.10）は容易には線形に変換できない

よって、これは「パラメータ（母数）における非線形性」である

<br />
仮定2（パラメータにおける線形性）では、**このようなモデルが禁止されている**<br />（Gujarati, 2003, p.42）

------------------------------------------------------------------------

表7.10
```{r echo=TRUE}
set.seed(1); n1 <- 1000
b0 <- 1; b1 <- 0.6; b2 <- 0.4
x3 <- runif(n1); x4 <- runif(n1); e1 <- rnorm(n1)
y5 <- b0 * x3^b1 * x4^b2 * exp(e1)
y6 <- b0 * x3^b1 * x4^b2 + e1
model8 <- lm(y5 ~ x3 + x4)
model9 <- lm(log(y5) ~ log(x3) + log(x4))
model10 <- lm(y6 ~ x3 + x4)
model11 <- lm(log(y6) ~ log(x3) + log(x4))
```


表7.11

| 　 | モデル8 | モデル9 | モデル10 | モデル11 |
|:---:|----:|----:|----:|----:|
|$\hat{\beta_{1}}$|0.981|0.600|0.502|0.063|
|$\hat{\beta_{2}}$|0.907|0.451|0.625|0.110|

表7.11は出力を抜粋したものである

モデル8では、$\hat{\beta_{1}}$ と $\hat{\beta_{2}}$ は、それぞれ0.981と0.907で、真値の0.6と0.4とは大きく異なっている

モデル9では、$\hat{\beta_{1}}$ と $\hat{\beta_{2}}$ は、それぞれ0.600と0.451で、真値の0.6と0.4とかなり近い

細かな違いは標本抽出誤差である

モデル10とモデル11は誤った結果である

------------------------------------------------------------------------

特に、モデル11に関しては、警告メッセージが出ている

これは、結果変数が負の値を含んでおり、真数条件により、対数に変換できないからである

実際に、y6の最初の5個を表示してみると、3番目の値が-0.383であり、

これを自然対数に変換した値はNaN（Not a Number）となっている

すなわち、式（7.10）は、パラメータにおいて非線形であるため、

通常の最小二乗法（OLS）によって適切に推定できないのである



## 7.2.6 多変量における診断方法 {#Ch_7_2_6}

ここまで、簡単のため、二変量に限定して議論してきた

単回帰モデルの場合、$X$ と $Y$ の散布図から視覚的に関数の形を把握することができる

しかし、共変量が多変量である重回帰分析では、他の共変量を統制した場合の効果（偏回帰係数）に興味があるため、二変量の散布図では、適切な関数の形を探すことができない

<br />
そこで、成分プラス残差プロット（component-plus-residual plot）を使用することが推奨されている（Fox, 2020, p.83）

このグラフは偏残差プロット（partial residual plot）とも呼ばれる<br />
（Meuleman, Loosveldt, and Emonds, 2015, p.89）

------------------------------------------------------------------------

具体的に、式（7.12）の三変量の場合を考えてみる

$x_{1i}$ と $x_{2i}$ の平均値はそれぞれ5、分散はそれぞれ1であり、相関係数は0.5の二変量正規分布に従うとする

$x_{3i}=\exp(x_{2i})$ である

$\varepsilon_{i} \approx N(0, 0.01)$ である

$$
y_{1i} = \exp[1.0 + 0.5 x_{1i} + 0.2 \log(x_{3i}) + \varepsilon_{i}] \tag{7.12}
$$

<br />
$j$ 番目の説明変数の偏残差（partial residual）を式（7.13）のとおり定義する

ここで、$b_{j}x_{ji}$ は $y$ と $x_{j}$ との部分関係（partial relationship）を表し、$e_{j}$ は通常の残差を表す

成分プラス残差プロットは、$x_{j}$ を横軸に配置し、$e_{ji}$ を縦軸に配置する散布図である

$$
e_{ji} = b_{j} x_{ji} + e_{j} \tag{7.13}
$$

------------------------------------------------------------------------

表7.12
```{r echo=TRUE}
rm(list = ls())
data07b <- read.csv("./causality-main/data07b.csv")
summary(data07b)
```

------------------------------------------------------------------------


表7.13
```{r echo=TRUE}
library(car)
model12 <- lm(y1 ~ x1 + x3, data=data07b)
model13 <- lm(y1 ~ log(x1) + log(x3), data=data07b)
model14 <- lm(log(y1) ~ x1 + x3, data=data07b)
model15 <- lm(log(y1) ~ log(x1) + log(x3), data=data07b)
```

------------------------------------------------------------------------

```{r echo=TRUE}
crPlots(model12)
```

図7.1 - 1

------------------------------------------------------------------------

```{r echo=TRUE}
crPlots(model13)
```

図7.1 - 2

------------------------------------------------------------------------

```{r echo=TRUE}
crPlots(model14)
```

図7.1 - 3

------------------------------------------------------------------------

```{r echo=TRUE}
crPlots(model15)
```

図7.1 - 4

------------------------------------------------------------------------

出力結果は、図7.1のとおりである

図7.1は、ノンパラメトリックなloess曲線によって関数形を表現したものである

この曲線が、より線形に見えるように変換されていればよい

図7.1では、1～4まで4組（計8枚）の成分プラス残差プロットが表示されている

------------------------------------------------------------------------

図7.1-1はモデル12で、すべての変数が何も変換されていない状態である

いずれの関係も非線形である

変数x3とy1のloess曲線は直線に見えるが、○印で表されているデータの値を見ると、線形でなく単に外れ値と引っ張られているだけである

<br />
図7.1-2はモデル13で、x1とx3は自然対数に変換されているが、y1は変換していない状態である

ここでも、非線形の関係である

<br />
図7.1-3はモデル14で、y1のみ自然対数に変換されているが、x1とx3は変換されていない状態である

x1とlog(y1)の関係は線形に見えるが、x3とlog(y1)の関係は非線形である

<br />
図7.1-4はモデル15で、すべての変数が自然対数に変換されている状態である

log(x1)とlog(y1)の関係は非線形であるが、log(x3)とlog(y1)の関係は線形に見える

よって、以上の組合せの中から、log(y1)、x1、log(x3)と変換すればよいことがわかる

------------------------------------------------------------------------

モデル16は、その結果である

b1の真値は0.5に設定されており、実際に0.501と正確に推定できている

b2の真値は0.2に設定されており、実際に0.205と正確に推定できている

```{r echo=TRUE}
model16 <- lm(log(y1) ~ x1 + log(x3), data=data07b)
summary(model16)
```

------------------------------------------------------------------------

以上のとおり、**重回帰モデルから平均処置効果（ATE）を推定するには、関数形を正しく設定することが重要**である

ただし、成分プラス残差プロットを使用するには、組合わせを解析者が考えて実行する必要がある

<br />
ゆえに、**変数が増えてくると組み合わせを考えてモデルを組み上げる手間が膨大なものとなる**

<br />
さらに、今回は関数形の候補として変換なしと対数変換の2つのみを考慮したが、考え得る関数形の候補は無数にある

これは本質的に重要なポイントであり、9.4節および9.5節で改めて議論する



# 7.3 仮定 3：誤差項の条件付き期待値ゼロ {#Ch_7_3}

## 7.3 仮定 3：誤差項の条件付き期待値ゼロ {#Ch_7_3_0}

誤差項 $\varepsilon_{i}$ と共変量 $X$ は独立と仮定する

すなわち、3.4節で独立性と条件付き期待値について確認したとおり、<br />
$E[\varepsilon_{i}|X] = E[\varepsilon_{i}]$ である

この仮定が満たされているとき、誤差項 $\varepsilon_{i}$ は共変量 $X$ と平均独立（mean independent）という

また、仮定1（誤差項の期待値ゼロ）より、$E[\varepsilon_{i}] = 0$ である

つまり、式（7.14）のとおり、共変量 $X$ が与えられたとき、誤差項 $\varepsilon_{i}$ は共変量 $X$ と条件付き平均独立（conditional mean independent）であるという

$$
E[\varepsilon_{i}|X] = 0 \tag{7.14}
$$

<br />
この仮定は、最小二乗法による回帰係数の推定量が不偏であるために必要な仮定であり、<br />
統計的因果推論において、非常に重要である

------------------------------------------------------------------------

第6章で見たとおり、説明変数 $X_{1}$ と共変量 $X_{2}$ に相関があり、

$X_{2}$ をモデルに含まない場合、$X_{1}$ から $Y$ への効果には交絡が起こっていた

また、$X_{2}$ をモデルに含めることで、重回帰モデルでは、$X_{1}$ から $Y$ への純粋な効果を推定できた

仮定3（誤差項の条件付き期待値ゼロ）が述べていることは、

**統制すべき交絡因子が十分にモデルに含まれていなければならないこと**

である

<br />
残念ながら、これを診断する方法はない

<br />
まず、誤差項 $\varepsilon_{i}$ は観測されないため、直接的に検証することはできない

<br />
次に、残差 $e_{i}$ と共変量 $X$ との関係を調べることを思いつくかもしれないが、

残差と説明変数は、最小二乗法の原理によって、そもそも相関がないので、

自動的に $E[e_{i}|X] = 0$ となってしまうのである（Fox, 2020, p.7）

------------------------------------------------------------------------

統計的因果推論の立場からは、3.8節で確認したとおり、

式（7.15）が成り立っていることを無視可能な割付けといった

観測された共変量の値が同じ個体同士では、処置の割り付けは無作為化されていると

考えてよいという仮定である

<br />
すなわち、共変量 $X$ に十分な変数が含まれているかどうかが重要であった

$$
Pr(T_{i} | Y_{i}(1), Y_{i}(0), X) = Pr(T_{i}|X) \tag{7.15}
$$

<br />
この仮定を満たすためには、できるだけ多くの変数をモデルに取り入れて、

必要な共変量を取りこぼす可能性を下げる必要がある

<br />
ここで、データセット内にある変数は、すべてモデル取り込んでしまっていいのかという疑問があるだろう

検討すべきことは主に2点ある

- 1点目は、不要な変数をモデルに取り入れることの影響である

- 2点目は、因果関係の間に位置する変数の取り扱いである


## 7.3.1 不要な変数をモデルに取り入れる問題 {#Ch_7_3_1}

式（7.16）の四変量モデルを考える

$\beta_{0} = 1.0$、$\beta_{1} = 1.3$、$\beta_{2} = 1.2$、$\beta_{3}=0.0$、$\varepsilon_{i} \sim N(0, 1)$ とする

$X_{1}$ から $Y$ への因果効果 $\beta_{1}$ に興味があるとする

このとき、$\beta_{3}=0.0$ なので、$X_{3}$ は不要な変数である

このように、不要な変数 $X_{3}$ をモデルに取り込んだ場合、$\beta_{1}$ の推定にどのような影響が出るかが1つ目の問題である

$$
Y_{i} = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} X_{2i} + \beta_{3} X_{3i} + \varepsilon_{i} \tag{7.16}
$$

<br />
結論を先に述べると、$\beta_{1}$ の推定は偏りなく行うことができる(Wooldridge, 2020, p.84)

ただし、$X_{1}$ と $X_{3}$ の相関が強くなると、標準誤差が大きくなるおそれがある<br />
(Wooldridge, 2020, p.93)

<br />
不要とわかっているならば、モデルに入れる必要がないことは明らかである

しかし、実証研究では、ある共変量が実際に必要なのか、不要なのか、わからないことが多い

不要な変数をモデルに取り入れることには、不偏性という点からは大きな問題はないが、標準誤差に悪影響が出るおそれはある

------------------------------------------------------------------------

表7.15
```{r echo=TRUE}
rm(list = ls())
data07c <- read.csv("./causality-main/data07c.csv")
summary(data07c)
```

------------------------------------------------------------------------

表7.16のとおり、3つのモデルを作成する

1行目は単回帰モデルであり、必要な変数x2が含まれておらず、不要な変数x3も含めていない

2行目は重回帰モデルであり、必要な変数x2が含まれており、不要な変数x3は含まれていない

3行目も重回帰モデルであるが、不要な変数x3も含めてデータをすべて使っている

表7.16
```{r echo=TRUE}
model17 <- lm(y1 ~ x1, data=data07c)
model18 <- lm(y1 ~ x1 + x2, data=data07c)
model19 <- lm(y1 ~ x1 + x2 + x3, data=data07c)

summary(model17)
```

------------------------------------------------------------------------

```{r echo=TRUE}
summary(model18)

summary(model19)
```

------------------------------------------------------------------------

$\beta_{1} = 1.3$ であるから、モデル1の推定結果1.903は誤りである

モデル2の推定結果1.334と、モデル3の推定結果1.333は正しい結果といえる

つまり、不要な変数x3をモデルに含めても偏りは発生していない様子がわかる

<br />

では、不要な変数x3をモデルに含めることの悪影響はないのだろうか？

表7.17では、モデル18とモデル19の95%信頼区間を計算している

モデル17については、そもそも点推定値が誤っているので、ここでは信頼区間の検討はしていない

表7.17
```{r echo=TRUE}
confint(model18)
confint(model19)
```

------------------------------------------------------------------------

モデル18における $\hat{\beta_{1}}$ の95%信頼区間は、1.263～1.404である

一方、モデル19における $\hat{\beta_{1}}$ の95%信頼区間は、1.188～1.479である

どちらも真値 $\beta_{1} = 1.3$ を区間の中に含んでいることから、正しい結果ではあるが、

**モデル19の信頼区間の方が、モデル18の信頼区間よりも幅が大きい**

これは、不要な変数x3を取り込んだ結果、推定結果にノイズが混ざったため、標準誤差に悪影響が出たからである

つまり、推定の精度に影響があり得る

先ほどの出力結果を見てみると、モデル2の標準誤差は0.036だったが、モデル3の標準誤差は0.074となっており、大きくなっていた

<br />
よって、不要な変数をモデルに取り入れても偏りはないため、ある変数が必要かどうか判断に迷う場合は、入れる方がよい

ただし、その変数が実際には不要だった場合、偏りに問題がないものの、推定の精度に影響が出ることには注意が必要であり、信頼区間が必要以上に大きくなっているおそれはある

------------------------------------------------------------------------

情報量基準（information Criterion）による変数選択についても言及する

松浦（2016, p.25）が指摘するとおり、統計的因果推論の立場からは、情報量基準による変数選択は推奨されない

以下は、赤池情報量基準（AIC：Akaike Information Criterion）を用いて変数選択するR関数stepAICの問題点をシミュレーションした結果である

表7.9と同じデータ生成プロセスを用いて、1万回のモンテカルロ・シミュレーション（Monte Carlo simulation）を実行した

------------------------------------------------------------------------

表7.18

<center>

<img src="./img/list_7_18.png" style="border: none;">

</center>

モデルAICは、R関数stepAICによって変数選択をしたときのモデルであり、AICの値に応じて、表7.16のモデル18またはモデル19を選んでいる

モデル19はx1、x2、x3をモデルに含んでいる重回帰モデルで、表7.16のモデル19と同じである

$\beta_{3}=0.00$ のとき、真のモデルは $\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}X_{1i} + \hat{\beta_{2}}X_{2i}$ であるからモデル18から不偏推定できるはずである

また、$\beta_{3} = 0.05$ のとき、真のモデルは$\hat{Y_{i}} = \hat{\beta_{0}} + \hat{\beta_{1}}X_{1i} + \hat{\beta_{2}}X_{2i} + \hat{\beta_{3}}X_{3i}$であるから、モデル19から不偏推定できるはずである

------------------------------------------------------------------------

まず、$\beta_{3}=0.00$ の場合を見てみよう

共変量x3に一切の効果がない場合には、どちらのモデルも不偏であり、モデルAICの標準誤差の方がモデル19と比べてわずかに小さい

標準誤差が小さいことは、有効性（efficiency）があることを意味しているが、カバー率を確認すると、必ずしも良い結果とはいえないことがわかる

カバー率とは、**繰り返し演算の中で、名目95%信頼区間の中に $\beta_{1}$ の真値1.3が何%含まれていたか**を示している

名目95%なので、95%に近いほど良い結果を残す

モデルAICのカバー率は91.31%しかないのに対し、モデル19のカバー率は94.97%で名目通りに正しい

モデル19の区間は長くなっているが、このおかげで名目どおりのカバー率を達成できている

一方、$\beta_{3}=0.05$ の場合、つまりx3の効果がわずかに存在する場合には、モデルAICの結果は偏っているが、モデル19には偏りがない

2つのモデルの間に標準誤差の差はほとんどないが、カバー率という点ではモデル9が圧倒的に優れている

モデルAICの結果はそもそも点推定値が偏っているため、信頼区間のカバー率も低いのである

------------------------------------------------------------------------

すなわち、**統計的因果推論の立場からは、AICによる変数選択を行わない方がよい**ことが分かる

また、結果変数に対してほとんど影響を持たないと思われる不要そうな変数であっても、モデルに取り入れてよいことも示されている


## 7.3.2 中間変数をモデルに取り入れる問題 {#Ch_7_3_2}

次に「因果関係の間に位置する変数」の問題を考える（岩崎, 2015, p.56; 林・黒木, 2016, p.39）

結論から述べると、**このような変数はモデルに含めてはならない**

<br />
この問題は、方向付き非巡回グラフ（DAG）を使うとわかりやすい

$X$ が原因で、$Y$ が結果であるとき、$X \rightarrow Y$ と表すものであった

図7.2では、$X_{1}$ が $Y$ の原因と考えている

6.8節のDAGでは $X_{1} \leftarrow X_{2}$ となっていたが、ここでは $X_{1} \rightarrow X_{2}$ となっている点に注意が必要である

<center>

<img src="./img/pic_7_2.png" style="border: none;">

図7.2
</center>

------------------------------------------------------------------------

すなわち、$X_{2}$ は、$X_{1}$ と $Y$ の間にある

因果関係の間に位置する $X_{2}$ のような変数を**中間変数（mediator）**といい、共変量とは別のものとして考える（shadish et al., p.509）

$X_{1} \rightarrow Y$ の部分を**直接効果（direct effect）**といい、$X_{1} \rightarrow X_{2} \rightarrow Y$ の部分を**間接効果（indirect effect）**という

全体の効果（total effect）は、直接効果と間接効果を合わせたものである

<br />
図7.2を式で表すと式（7.17）および式（7.18）のようになる

$$
\begin{align}
X_{2i} & = \gamma_{0} + \gamma_{1} X_{1i} + \varepsilon_{1i} \tag{7.17} \\
Y_{1i} & = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} X_{2i} + \varepsilon_{2i} \tag{7.18} \\
\end{align}
$$

ここで、式（7.18）の $X_{2}$ に式（7.17）を入力することで、全体の効果がどのように表されるのかがわかる

------------------------------------------------------------------------

式（7.19)の最終行のとおり、$X_{1}$ から$Y$ への全体の効果は $\beta_{1} + \beta_{2} \gamma_{1}$ である

これが推定対象である

$$
\begin{align}
Y_{1i} & = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} X_{2i} + \varepsilon_{2i} \\
Y_{1i} & = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} (\gamma_{0} + \gamma_{1} X_{1i} + \varepsilon_{1i}) + \varepsilon_{2i} \\
Y_{1i} & = \beta_{0} + \beta_{1} X_{1i} + \beta_{2} \gamma_{0} + \beta_{2} \gamma_{1} X_{1i} + \beta_{2}  \varepsilon_{1i} + \varepsilon_{2i} \\
Y_{1i} & = (\beta_{0} + \beta_{2} \gamma_{0}) + (\beta_{1} + \beta_{2} \gamma_{1}) X_{1i} + (\beta_{2}  \varepsilon_{1i} + \varepsilon_{2i})  \tag{7.19}
\end{align}
$$

<br />
中間変数の何が問題であるかを具体的に確認する

表7.19
```{r echo=TRUE}
rm(list = ls())
data07d <- read.csv("./causality-main/data07d.csv")
summary(data07d)
```

<br />
このデータではx1、e1、e2は標準正規乱数として生成しており、この3つは独立である

------------------------------------------------------------------------


次に、x2 = 1.0 + 1.5 * x1 + e1 として生成している

つまり、x2はx1の影響を受けており、その影響は $\gamma_{1} = 1.5$ である

次に、y1 = 1.0 + 1.3 * x1 + 1.2 * x2 + e2 として生成している

つまり、y1はx1からの直接の効果 $\beta_{1} = 1.3$を受けているが、x2もy1に対して $\beta_{2} = 1.2$の効果を与えている

この状況設定と図7.2を見比べて、中間変数の問題設定を理解するように努めてほしい

表7.20のとおり、中間変数x2を含まないモデル20と、中間変数x2を含むモデル21を推定する

表7.20
```{r echo=TRUE}
model20 <- lm(y1 ~ x1, data=data07d)
model21 <- lm(y1 ~ x1 + x2, data=data07d)
```

------------------------------------------------------------------------

```{r echo=TRUE}
summary(model20)
```


```{r echo=TRUE}
summary(model21)
```

------------------------------------------------------------------------

推定すべき因果効果の真値は3.1であった

モデル20では、3.157であるから、正しく推定できている

一方、モデル21では、1.316であり、正しく推定できていない

ここで注意すべきことは、1.316は $\beta_{1}$ の値として正しく、これは$X_{1} \rightarrow Y$ の<br />
直接効果を表している

しかしながら、推定すべき因果効果は、直接効果と間接効果を合わせた全体の効果であり、<br />
これは $\beta_{1} + \beta_{2} \gamma_{1} = 3.1$ の方である

<br />
結論としては、

**中間変数をモデルに含めてしまうと、推定対象である全体の効果を適切に推定できなくなってしまう**

といえる

